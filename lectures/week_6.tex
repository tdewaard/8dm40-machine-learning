\documentclass[notes]{beamer}          % print frame + notes
%\documentclass[notes=only]{beamer}     % only notes
%\documentclass{beamer}                 % only frames
 
\usecolortheme{beaver}

% Some commonly used packages
% (copied mainly from the Utrecht University theme: https://www.overleaf.com/project/5c900fa3bd9930036341116a)
\usepackage{ragged2e}  % `\justifying` text
\usepackage{booktabs}  % Tables
\usepackage{tabularx}
\usepackage{tikz}      % Diagrams
\usetikzlibrary{calc, shapes, backgrounds}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{url}       % `\url`s
\usepackage{listings}  % Code listings
\usepackage{comment}  
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}

% Mainly math commands
\newcommand{\vect}[1]{\bm{#1}}
\usepackage{amsfonts}% to get the \mathbb alphabet
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\C}{\field{C}}
\newcommand{\R}{\field{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

% A variable used to exclude slides from the lecture version
\newif\iffull
%\fullfalse
\fulltrue

% Bibliography 
\usepackage[uniquename=init,giveninits=true,maxcitenames=1,style=authortitle-comp]{biblatex}
\bibliography{references}


%Information to be included in the title page:
\title{Classification: Support vector machines and random forests}
\author{Federica Eduati}
\institute{Eindhoven University of Technology

Department of Biomedical Engineering}
\date{2019}
 
 
\begin{document}
 
\frame{\titlepage}
 
\section{Overview}

\begin{frame}
\frametitle{Learning goals}
At the end of this lecture you will:
\begin{itemize}
    \item Have a general understanding of support vector machines for classification.
    \item Have a general understanding of machine learning methods based on decision trees (including random forests).
\end{itemize}

\vspace{5mm} 

Materials: 
\begin{itemize}
    \item Chapters 9, 12 and 15 from \cite{elements}
\end{itemize}

\end{frame}


\section{General introduction}
\begin{frame}{Maximal margin classifier}

Classification problem: find a hyperplane that separates the classes in feature space.

\vspace{5mm} 

In $p$ dimensions a hyperplane is a flat affine subspace of dimension $p-1$, with general equation.

\begin{equation}
    f(x) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = x^T\beta + \beta_0 = 0
\end{equation}

Where:
\begin{itemize}
    \item $\beta_0 = 0$ only if the hyperplane goes through the origin \item the vector $\beta = (\beta_1, \beta_2, \dots, \beta_p)$ is a unit vector ($\norm{\beta} = 1$) orthogonal to the surface of the hyperplane
\end{itemize}
    
\end{frame}


\begin{frame}{Maximal margin classifier}
Imagine to have a training data of N pairs: $\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$, with $x_i \in {\rm I\!R}^p$ and $y_i \in \left\{-1,1 \right\}$.

If the classes are perfectly separable, there are generally multiple hyperplanes that can separate them.

\begin{center}
\includegraphics[height=5.5cm]{figures/week_6/svm_hyperplanes.pdf}
\end{center}

\end{frame}


\begin{frame}{Maximal margin classifier}
The \textit{maximal margin classifier} is the one with biggest margin between the two classes.

\begin{center}
\includegraphics[height=5.5cm]{figures/week_6/svm_maximal_margin_classifier.pdf}
\end{center}

\vspace{-7mm} 

\begin{equation*}
\max_{\beta, \beta_0, \norm{\beta} = 1} M \text{, subject to } y_i(x^T_i \beta + \beta_0) \geq M, i=1, \dots, N
\end{equation*}

\end{frame}

\begin{frame}{Noisy or non-separable data}
The maximal margin classifier has issues in case of:
\begin{itemize}
    \item Noisy data with outliers leading to poor solution (left panel - just added one data point to the previous example).
    \item Data non-separable by linear boundary (right panel).
\end{itemize}

\begin{center}
\includegraphics[height=5cm]{figures/week_6/svm_outlier_or_not_separable.pdf}  
\end{center}
    
\end{frame}


\begin{frame}{Support vector classifier}

The \textit{support vector classifier} provides a solution by maximising a \textit{soft} margin (regularization).

\vspace{5mm}

For this we can modify the optimization problem allowing some slack.

\begin{equation*}
\max_{\beta, \beta_0, \norm{\beta} = 1} M \text{, subject to } y_i(x^T_i \beta + \beta_0) \geq M (1-\xi_i), i=1, \dots, N
\end{equation*}

where $\xi_i \geq 0$ and $\sum_{i=1}^N \xi_i \leq C$. $C$ is a constant that defines the budget we allow for the total amount of slack.

\vspace{5mm}



\end{frame}


\begin{frame}{Support vector classifier}

The \textit{support vector classifier} provides a solution by maximising a \textit{soft} margin.
\begin{center}
\includegraphics[height=5.5cm]{figures/week_6/svm_support_vector_classifier.pdf}  
\end{center}

\vspace{-7mm} 

\begin{equation*}
\max_{\beta, \beta_0, \norm{\beta} = 1} M \text{, subject to } y_i(x^T_i \beta + \beta_0) \geq M (1-\xi_i), i=1, \dots, N
\end{equation*}

\end{frame}

\begin{frame}{Support vector classifier: slack variables}

The slack variables $\xi = (\xi_1, \xi_2, \dots, \xi_N)$ tell us how much each point is allowed to be on the wrong side of its margin (relative amount).

\begin{itemize}
    \item $\xi = 0$ when the $i$th observation is on the correct side of the margin
    \item $\xi > 0$ when the $i$th observation is on the wrong side of the margin
    \item $\xi > 1$ when the $i$th observation is on the wrong side of the hyperplane
\end{itemize}


\end{frame}


\begin{frame}{Support vector classifier: regularization}

The constant $C$ (slack budget) is tunable and can be seen as a regularization parameter.

\begin{itemize}
    \item $C = 0$ no budget for violation of the margin (maximum margin classifier)
    \item increasing $C$ allows more slack allowed (wider margins)
\end{itemize}

Therefore $C$\ controls the bias-variance trade-off:

\begin{itemize}
    \item small $C$ $\rightarrow$ narrow margins $\rightarrow$ high fit to the data $\rightarrow$ low bias, high variance
    \item large $C$ $\rightarrow$ wide margins $\rightarrow$ more violation allowed $\rightarrow$ high bias, low variance
\end{itemize}

\end{frame}

\begin{frame}{Support vector classifier: example}

Example of support vector classifier for increasing values of $C$.

\begin{center}
\includegraphics[height=4.2cm]{figures/week_6/svm_support_vector_classifier_regularization_C.pdf}  
\end{center}

The \textit{support points} (marked with diamonds), i.e. those with $\xi_i \neq 0$, are the only ones that determine the orientation of the margin.

\end{frame}

\begin{frame}{Support vector classifier: noisy and non-separable data}

The support vector classifier allows to have a good classifier in both the examples of noisy and non-separable data that we have seen earlier, where the maximal margin classifier was not working properly.

\begin{center}
\includegraphics[height=5cm]{figures/week_6/svm_outlier_or_not_separable_support_vector_classifier.pdf}  
\end{center}

\end{frame}

\section{Support vector machine}
\begin{frame}{Support Vector Machines: Classification with non-linear decision boundaries}
Extension of the Support vector classifier to handle \textbf{non-linear class boundaries}. 
\begin{itemize}
 \item Linear regression: use of quadratic and cubic terms.
 \item Support vector classifier: use of quadratic, cubic, and even higher-order polynomial functions of the predictors.
\end{itemize}
For instance, from p features to 2p features:
\begin{center}{
$X_1, X_2, \dots, X_P,$ \hspace{0.05\textwidth}\to\hspace{0.05\textwidth} $X_1, X^2_1, X_2, X^2_2, \dots, X_P, X^2_P$}
\end{center}\\
Then, the optimization problem would become:\useshortskip
\begin{equation*}
\max_{\beta, \beta_0, \norm{\beta} = 1} M \text{, subject to } y_i(x^T_i \beta + {x^2_i}^T \beta+ \beta_0) \geq M (1-\xi_i), i=1, \dots, N
\end{equation*}
where $\xi_i \geq 0$ and $\sum_{i=1}^N \xi_i \leq C$.
\end{frame}
\section{Support vector machine}
\begin{frame}{Support Vector Machines: The Support Vector Machine}

Why does this lead to a non-linear decision boundary?
\begin{itemize}
 \item Enlarged feature space: linear decision boundary
 \item Original feature space: non-linear (quadratic polynomial)
\end{itemize}
\begin{figure}
  \includegraphics[width=0.8\textwidth]{figures/week_6/SVM_kernel.pdf}  
\end{figure}
Many possible ways to enlarge the feature space.
\begin{itemize}
 \item Be careful with large number of features (unmanageable computations)\\
\end{itemize}

The \textit{support vector machine}(SVM): extension of the support vector classifier which enlarges the feature space by using \textbf{kernels}.\\

Here, the kernel approach is a efficient computational methodology to enlarge the feature space.
\end{frame}

\section{Support vector machine}
\begin{frame}{Support Vector Machines: The Support Vector Machine}
Inner product definition:$\langle {a,b} \rangle = \sum_{i=1}^{r} a_{i}b_{i},$ where a,b are r-vectors\\~\\
The solution to the support vector classifier problem involves only the inner products of the observations. The inner product of two observations $x_i, x_i'$ is given by
\begin{equation*}
\langle {x_i,x_i'} \rangle = \sum_{j=1}^{p} x_{ij} x_{i'j}
\end{equation*}
Then, the linear support vector classifier can be represented as:
\begin{equation*}
f(x) = \beta_0 + \sum_{i=1}^{n} \alpha_i \langle {x,x_i} \rangle, i=1, \dots, N
\end{equation*}
where there are \textit{n} parameters $\alpha_i$, one per training observation.
\end{frame}

\section{Support vector machine}
\begin{frame}{Support Vector Machines: The Support Vector Machine}
$\alpha_i$ is nonzero only for the support vectors. \\~\\
So if \mathit{S} is the collection of indices of these support points, we can rewrite f(x) which involves far fewer terms than before:
\begin{equation*}
f(x) = \beta_0 + \sum_{i\in\mathit{S}} \alpha_i \langle {x,x_i} \rangle,
\end{equation*}
Generalization of the inner product of the form:
\begin{equation*}
\mathit{K}(x_i,x_i'),
\end{equation*}
where we refer to $\mathit{K}$ as \textit{kernel}. 
A \textit{kernel} is a function that quantifies the similarity of two observations.
\end{frame}
\section{Support vector machine}
\begin{frame}{Support Vector Machines: The Support Vector Machine}
Linear kernel: $\mathit{K}(x_i,x_i') = \sum_{j=1}^{p} x_{ij} x_{i'j}$ \\
Polynomial kernel of degree d: $\mathit{K}(x_i,x_i') = (1+ \sum_{j=1}^{p} x_{ij} x_{i'j})^d$\\
Radial kernel: $\mathit{K}(x_i,x_i') = exp(- \gamma \sum_{j=1}^{p} (x_{ij}-x_{i'j})^2)$ \\~\\
What is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features?
\begin{itemize}
 \item Computational advantage. We don't work in the enlarged feature space.
\end{itemize}

\end{frame}
\section{Extension to multi-class}
\begin{frame}{Extension to multi-class}
So far, binary classification, in other words, two-class setting. \\~\\
SVMs: concept of separating hyperplanes does not lend itself to more than two classes. \\~\\
Two approaches for extending SVMs to K$>$2 classes classification:
\begin{itemize}
 \item One-Versus-One Classification: $\binom{K}{2}$ SVMs comparing pair of classes. We assign the test observation to the class most frequently selected in these pairwise classification.
 \item One-Versus-All Classification: K SVMs, each time comparing one of the K classes to the remaining K-1 classes. We assign the test observation to the class (SVM in this case) with the best discrimination rule. 
\end{itemize}


\end{frame}

\section{Tree-based methods}
\begin{frame}{Decision tree}
What does a decision tree represent?
\begin{figure}
\minipage{0.22\textwidth}
  \includegraphics[width=\linewidth]{figures/week_6/tree-example-a.pdf}  
\endminipage\hfill
\minipage{0.26\textwidth}
  \includegraphics[width=\linewidth]{figures/week_6/tree-example-b.pdf}  
\endminipage\hfill
\minipage{0.46\textwidth}%
  \includegraphics[width=\linewidth]{figures/week_6/tree-example-c.pdf}  
\endminipage
\end{figure}

How do we know what the optimal splitting point is at each node? \newline

Objective function: maximize the \textbf{Information Gain (IG)}
\begin{equation*}
IG(D_p,f) = I(D_p) - (\frac{N_{left}}{N_p}I(D_{left}) + \frac{N_{right}}{N_p} I(D_{right}))
\end{equation*}

The lower the impurity of the child nodes, the larger the information gain.

\end{frame}

\begin{frame}{Classification Trees}
Classification problem: a class vote for each tree, and then classifies using majority vote. \newline

As impurity metric or splitting criteria: 
\begin{equation*}
\pushleft{\textbf{Entropy}}: E(t) = - \sum_{i=1}^{c} p(i|t) log_2 p(i|t)
\end{equation*}
\begin{equation*}
\pushleft{\textbf{Gini impurity}}: i(t) = 1 - \sum_{i=1}^{c} p^2(i|t) 
\end{equation*}
where $p(i|t)$ is the proportion of the samples that belong to class c for node t. 

\end{frame}
\begin{frame}{Regression Trees}
Regression problem: the predictions from each tree at a target point $x$ are simply averaged. \newline

As impurity metric or splitting criteria: 
\begin{equation*}
\pushleft{\textbf{Mean square error}}: MSE(t) = - \frac{1}{N_t} \sum_{i \epsilon D_t} (y^{(i)} - \hat{y}^t)^2
\end{equation*}
where $N_t$ is the number of training samples at node $t$, $D_t$ is the training subset at node $t$, $y(i)$ is the true target value, and $\hat{y}^t$ is the predicted target value
\end{frame}

\begin{frame}{Bagging or bootstrap aggregation}
Decision trees can become much more powerful when used as ensembles. \newline
\begin{minipage}{0.5\textwidth}
\begin{figure}
\includegraphics[height=3cm]{figures/week_6/bagging.pdf}  
\end{figure} 
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
\begin{itemize}
 \item The samples are drawn with replacement.
 \item High-variance, low-bias procedures, such as trees. 
\end{itemize}
\end{minipage}

\begin{minipage}{0.5\textwidth}
\begin{figure}[H]
\includegraphics[height=4cm]{figures/week_6/Bias-Variance-Tradeoff.png}  
\end{figure} 
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
\begin{itemize}
 \item \textit{From Understanding the Bias-Variance tradeoff, by Scott Fortmann Roe.} \newline
 \item The entire forest will have lower variance but not at the cost of increasing the bias.
\end{itemize}
\end{minipage}

\end{frame}

\begin{frame}{Random Forests: motivation}

\textbf{A way of bagging decision trees}: combining the predictions of $n$ different models, each of which having profound different insights into the relationships of the data. \newline

\textbf{Trees} are appropiate for bagging: high-variance, low-bias procedures. \newline

\textbf{Random Forests} builds a large collection of de-correlated trees, and then averages them. It brings in the insights from each of them. So this idea is called \textbf{Ensembling}. \newline

\textbf{Ensemblubg} is a machine learning technique, both for regression and classification tasks. It can also be used for feature selection. \newline

\end{frame}

\begin{frame}{Random Forests: algorithm}

\begin{minipage}{0.5\textwidth}
\begin{figure}[Ht]
\includegraphics[height=4.5cm]{figures/week_6/random-forest.pdf}  
\end{figure} 
\end{minipage} \hfill
\begin{minipage}{0.48\textwidth}

Workflow: 
\begin{itemize}
    \item \color{blue}  \textbf{Bootstrap sampling}: \color{black}to grow the tree, a random subsample of the total dataset is used.
    \item \color{red} \textbf{Model building}: \color{black}a random subset of all features is chosen as a "splitter variable".
    \item \color{green}  \textbf{Bootstrap aggregating} 
\end{itemize}
Details: 
\begin{itemize}
    \item \textbf{Out of Bag Samples}
    \item \textbf{Variable Importance}
    \item \textbf{Overfitting} 
\end{itemize}

\end{minipage}

\end{frame}

\begin{frame}{Random Forests are popular}
Advantages: 
\begin{itemize}
    \item \textbf{Small sample size}
    \item \textbf{High-dimensional feature space}
    \item \textbf{Complex data structures}  \newline
\end{itemize} 

Applications: 
\begin{itemize}
 \item Predicting drug responses for cancer cell lines [\italic{Riddick et al., Bioinformatics, 2010.}] \newline
\end{itemize} \newline

Genomic characterizations $\rightarrow$ Large number of features \break $\rightarrow$ RF utilize the top features based on bootstrap aggregation \break $\rightarrow$ Good performance.

\end{frame}

\end{document}